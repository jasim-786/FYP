{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOoQxlObj5o7FZpaQE0nIhS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z-mx1ZuLshHE","executionInfo":{"status":"ok","timestamp":1733131812657,"user_tz":-300,"elapsed":847,"user":{"displayName":"Haider Ali Sammar","userId":"10714297956263525038"}},"outputId":"ac37428d-3604-498e-a97c-2cb5aa582106"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model weights loaded successfully!\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 260ms/step\n","Predicted Class: Yellow_Rust\n","Class Probabilities: [2.3803649e-02 2.0230493e-06 9.7619438e-01 3.9350665e-09 6.8693793e-09\n"," 5.7335963e-09 3.9057539e-09 3.0889986e-09 2.4611351e-09 3.2281235e-09]\n"]}],"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import load_img, img_to_array\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Conv2D, MaxPooling2D, DepthwiseConv2D, BatchNormalization, LeakyReLU, Concatenate, Input\n","\n","# 1. Define the Model Architecture (same as the one used during training)\n","def inceptionv3_base(input_shape=(224, 224, 3)):\n","    inputs = Input(shape=input_shape)\n","\n","    x = Conv2D(32, (3, 3), strides=(2, 2), padding='valid', activation=None)(inputs)\n","    x = LeakyReLU(alpha=0.1)(x)\n","    x = Conv2D(32, (3, 3), padding='valid', activation=None)(x)\n","    x = LeakyReLU(alpha=0.1)(x)\n","    x = Conv2D(64, (3, 3), padding='same', activation=None)(x)\n","    x = LeakyReLU(alpha=0.1)(x)\n","    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='valid')(x)\n","\n","    x = Conv2D(80, (1, 1), padding='valid', activation=None)(x)\n","    x = LeakyReLU(alpha=0.1)(x)\n","    x = Conv2D(192, (3, 3), padding='valid', activation=None)(x)\n","    x = LeakyReLU(alpha=0.1)(x)\n","    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='valid')(x)\n","\n","    x = GlobalAveragePooling2D()(x)\n","    return Model(inputs, x)\n","\n","def custom_cnn_base(input_shape=(224, 224, 3)):\n","    inputs = Input(shape=input_shape)\n","\n","    x = Conv2D(32, (3, 3), strides=(2, 2), padding='same')(inputs)\n","    x = BatchNormalization()(x)\n","    x = LeakyReLU(alpha=0.1)(x)\n","\n","    x = DepthwiseConv2D((3, 3), padding='same')(x)\n","    x = BatchNormalization()(x)\n","    x = LeakyReLU(alpha=0.1)(x)\n","    x = Conv2D(64, (1, 1), padding='same')(x)\n","    x = BatchNormalization()(x)\n","    x = LeakyReLU(alpha=0.1)(x)\n","\n","    x = DepthwiseConv2D((3, 3), strides=(2, 2), padding='same')(x)\n","    x = BatchNormalization()(x)\n","    x = LeakyReLU(alpha=0.1)(x)\n","    x = Conv2D(128, (1, 1), padding='same')(x)\n","    x = BatchNormalization()(x)\n","    x = LeakyReLU(alpha=0.1)(x)\n","\n","    x = GlobalAveragePooling2D()(x)\n","    return Model(inputs, x)\n","\n","inception_base = inceptionv3_base(input_shape=(224, 224, 3))\n","custom_cnn_base = custom_cnn_base(input_shape=(224, 224, 3))\n","\n","inputs = Input(shape=(224, 224, 3))\n","inception_features = inception_base(inputs)\n","custom_cnn_features = custom_cnn_base(inputs)\n","\n","x = Concatenate()([inception_features, custom_cnn_features])\n","x = Dense(1024, activation='relu')(x)\n","outputs = Dense(10, activation='softmax')(x)\n","\n","hybrid_model = Model(inputs, outputs)\n","\n","# 2. Load Weights\n","weights_path = \"/content/model_weights.weights.h5\"  # Replace with the correct path\n","hybrid_model.load_weights(weights_path)\n","print(\"Model weights loaded successfully!\")\n","\n","# 3. Predict on Input Image\n","def predict_image(image_path, model, class_names):\n","    # Load and preprocess the image\n","    image = load_img(image_path, target_size=(224, 224))  # Resize the image\n","    image_array = img_to_array(image) / 255.0  # Normalize the image\n","    image_array = np.expand_dims(image_array, axis=0)  # Add batch dimension\n","\n","    # Predict\n","    predictions = model.predict(image_array)\n","    predicted_class_idx = np.argmax(predictions)\n","    predicted_class = class_names[predicted_class_idx]\n","    return predicted_class, predictions[0]\n","\n","# Define class names (adjust based on your dataset)\n","class_names = ['Brown_Rust', 'Healthy', 'Yellow_Rust', 'Class4', 'Class5', 'Class6', 'Class7', 'Class8', 'Class9', 'Class10']\n","\n","# Path to the test image\n","test_image_path = \"/content/yellow_rust_271.png\"  # Replace with the actual image path\n","\n","# Make a prediction\n","predicted_class, probabilities = predict_image(test_image_path, hybrid_model, class_names)\n","\n","# Output the result\n","print(f\"Predicted Class: {predicted_class}\")\n","print(f\"Class Probabilities: {probabilities}\")\n"]}]}